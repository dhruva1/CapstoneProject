{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WQU Capstone project - Short-term trading strategy on G10 currencies\n",
    "## Notebook five - Putting previous steps together and Feature Engineering\n",
    "\n",
    "* Sergey Chigrinov - chigrinov.s.88@gmail.com\n",
    "* Dhruv Agrawal -  dhruva1@stanfordalumni.org\n",
    "* Man Sing Ho - mshoalbert@gmail.com\n",
    "\n",
    "### Jun-Aug-2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After these steps the data should be ready for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "#insert you own path or use relative path\n",
    "path_to_project = os.path.realpath('..') # r'C:\\WQU\\Capstone\\Working_files'\n",
    "sys.path.append(path_to_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from pandas.tseries.offsets import BMonthEnd\n",
    "from multiprocessing import cpu_count\n",
    "#from statsmodels.tsa.stattools import adfuller\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from WQUcapstoneCode.sampling import sampling\n",
    "from WQUcapstoneCode.labeling import labeling\n",
    "from WQUcapstoneCode.technical import technical\n",
    "from WQUcapstoneCode.fracdif.fracdif import frac_diff_ffd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-talk')\n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = ['AUD/USD','AUD/CAD','AUD/JPY','EUR/USD','GBP/USD','NZD/USD','USD/CAD', 'USD/JPY']\n",
    "max_holding_period = 5 #days \n",
    "ticks_multiplyer = 1. #arbitrary\n",
    "min_ret_target_vol_multiplier = 0.7 #\n",
    "cpus = cpu_count()-1\n",
    "d = 0.35 #fracdiff parameter. 1=simple first order differencing\n",
    "#we can use ADF to find non-stationary features, however, \n",
    "#fractionally differentiated featured may hold useful information for further analysis\n",
    "#therefore, we'll use default list of features to apply fracDiff\n",
    "non_stationary_feat = {'price','fast','slow','average','upper_band','lower_band','tenka_sen','kijun_sen','senkou_span_a','senkou_span_b'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load interest rate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import interbank interest rate data\n",
    "# --------------------------------------------------------------------------------\n",
    "input_path_ir_data = os.path.join(path_to_project, 'input_data', 'interbank_IR_3m.xlsx')\n",
    "ir = pd.read_excel(input_path_ir_data,sheet_name = 'consolidatedData')\n",
    "ir.index =[dt.datetime.strptime(date, '%Y-%m-%d %H:%M:%S') for date in ir.date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUD/USD: Data preparation in progres...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 19050/19050 [00:00<00:00, 1458562.11it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 11722/11722 [00:01<00:00, 8116.73it/s]\n",
      "2020-07-23 17:05:49.670781 100.0% applyPtSlOnT1 done after 0.04 minutes. Remaining 0.0 minutes..\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:26<00:00,  2.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUD/CAD: Data preparation in progres...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 19027/19027 [00:00<00:00, 1262877.57it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 11500/11500 [00:01<00:00, 6573.92it/s]\n",
      "2020-07-23 17:06:44.337599 100.0% applyPtSlOnT1 done after 0.05 minutes. Remaining 0.0 minutes..\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:27<00:00,  2.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUD/JPY: Data preparation in progres...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 19056/19056 [00:00<00:00, 1357380.86it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 11570/11570 [00:01<00:00, 6714.07it/s]\n",
      "2020-07-23 17:07:43.726736 100.0% applyPtSlOnT1 done after 0.05 minutes. Remaining 0.0 minutes..\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:27<00:00,  2.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EUR/USD: Data preparation in progres...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 19066/19066 [00:00<00:00, 760591.59it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 11090/11090 [00:01<00:00, 7008.73it/s]\n",
      "2020-07-23 17:08:41.488684 100.0% applyPtSlOnT1 done after 0.04 minutes. Remaining 0.0 minutes..\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:25<00:00,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBP/USD: Data preparation in progres...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 19031/19031 [00:00<00:00, 1265225.31it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 11346/11346 [00:01<00:00, 7623.28it/s]\n",
      "2020-07-23 17:09:35.897796 100.0% applyPtSlOnT1 done after 0.04 minutes. Remaining 0.0 minutes..\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:24<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NZD/USD: Data preparation in progres...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 19026/19026 [00:00<00:00, 1112129.16it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 11630/11630 [00:01<00:00, 7190.72it/s]\n",
      "2020-07-23 17:10:29.103056 100.0% applyPtSlOnT1 done after 0.04 minutes. Remaining 0.0 minutes..\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:27<00:00,  2.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USD/CAD: Data preparation in progres...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 19040/19040 [00:00<00:00, 1353987.69it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 11020/11020 [00:01<00:00, 7482.68it/s]\n",
      "2020-07-23 17:11:25.874638 100.0% applyPtSlOnT1 done after 0.05 minutes. Remaining 0.0 minutes..\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:26<00:00,  2.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USD/JPY: Data preparation in progres...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 19058/19058 [00:00<00:00, 1267100.67it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 11329/11329 [00:01<00:00, 7205.65it/s]\n",
      "2020-07-23 17:12:21.020461 100.0% applyPtSlOnT1 done after 0.04 minutes. Remaining 0.0 minutes..\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:28<00:00,  2.80s/it]\n"
     ]
    }
   ],
   "source": [
    "offset = BMonthEnd()\n",
    "\n",
    "for ticker in tickers:\n",
    "    \n",
    "    # Print ticket the loop is running\n",
    "    print (ticker+\": Data preparation in progres...\")\n",
    "    \n",
    "    # Import FX spot data\n",
    "    # --------------------------------------------------------------------------------\n",
    "    input_path = os.path.join(path_to_project, 'input_data', ''.join(ticker.split('/')) + '.csv')\n",
    "    pair = pd.read_csv(input_path)\n",
    "    pair.index =[dt.datetime.strptime(date, '%Y-%m-%d %H:%M:%S') for date in pair.date]\n",
    "    pair = pair.drop(columns=['date'])\n",
    "    \n",
    "    # Assign CCY1 and CCY2 from ticker\n",
    "    # --------------------------------------------------------------------------------\n",
    "    CCY1 = ticker[:3]\n",
    "    CCY2 = ticker[-3:]\n",
    "    \n",
    "    # calculate interest rate differentials\n",
    "    # --------------------------------------------------------------------------------\n",
    "    ir_d1 = pd.DataFrame(index=ir.index, columns=['ir_d1'])\n",
    "    ir_d1['ir_d1']= ir[CCY1] - ir[CCY2]\n",
    "        \n",
    "    m_ticks = ticks_multiplyer * pair.tickqty.sum()/pair.shape[0]\n",
    "    \n",
    "    #print('Sampling')\n",
    "    # --------------------------------------------------------------------------------\n",
    "    tick_df = sampling.sampled_bar_df(pair, 'tickqty',m_ticks)\n",
    "    \n",
    "    #print('Labeling')   \n",
    "    # --------------------------------------------------------------------------------    \n",
    "    dailyVol = labeling.getDailyVol(tick_df.bidclose) \n",
    "    dailyVol.name = 'volatility'\n",
    "    close = tick_df[['bidclose','askclose']]\n",
    "    close.columns = ['bid','ask']  \n",
    "    tEvents = labeling.getTEvents(close,h=dailyVol.mean())\n",
    "    t1 = labeling.addVerticalBarrier(tEvents, close, numDays=max_holding_period)\n",
    "    ptsl = [1, 1] #symmetric take-profit and stop-loss\n",
    "    target = dailyVol\n",
    "    # select minRet\n",
    "    minRet = dailyVol.mean()*min_ret_target_vol_multiplier\n",
    "    close = (close['bid'] + close['ask']) / 2 #to simplify we'll work with the mid price\n",
    "    events = labeling.getEvents(close, tEvents, ptsl, target, minRet, cpus, t1=t1)\n",
    "    labels = labeling.getBins(events, close)\n",
    "    labels = labeling.dropLabels(labels)\n",
    "    \n",
    "    #print('Calculating technical features')\n",
    "    ema = technical.EMA(close)\n",
    "    bb = technical.BollingerBands(close)\n",
    "    cci = technical.CCI(close)\n",
    "    so = technical.Stochastic(close)\n",
    "    wr = technical.wr(close)\n",
    "    ic = technical.Ichimoku(close)\n",
    "    rsi = technical.RSI(close)\n",
    "    \n",
    "    lagged_px = pd.concat([close.shift(1), close.shift(2),close.shift(12)],axis=1)\n",
    "    lagged_px.columns = ['T-1','T-2','T-12']\n",
    "    lagged_returns = pd.DataFrame(np.diff(np.log(lagged_px), axis=0), \n",
    "                                  index = lagged_px.index[1:], \n",
    "                                  columns=[c+'_1per_rtn' for c in lagged_px])\n",
    "    period_returns = pd.concat([np.log(close/close.shift(1)), \n",
    "                                np.log(close/close.shift(2)), \n",
    "                                np.log(close/close.shift(12))],axis=1)\n",
    "    period_returns.columns = ['T-1_rtn','T-2_rtn','T-12_rtn']\n",
    "    lags = pd.concat([technical.rolling_autocorr(close,lag=1),\n",
    "                      technical.rolling_autocorr(close,lag=2),\n",
    "                      technical.rolling_autocorr(close,lag=4),\n",
    "                      technical.rolling_autocorr(close,lag=6)], axis=1) #2,4,6 were found to be correlated earlier\n",
    "    \n",
    "    ema.data = ema.data.rename(columns={'side':'ema_side'})\n",
    "    bb.data = bb.data.rename(columns={'side':'bb_side'})\n",
    "    so.data = so.data.rename(columns={'side':'so_side'})\n",
    "    cci.data = cci.data.rename(columns={'side':'cci_side'})\n",
    "    wr.data = wr.data.rename(columns={'side':'wr_side'})\n",
    "    ic.data = ic.data.rename(columns={'side':'ic_side'})\n",
    "    rsi.data = rsi.data.rename(columns={'side':'rsi_side'})\n",
    "    feat = lambda x: [col for col in x.columns if col != 'price'] \n",
    "    features = pd.concat([ema(), \n",
    "                          bb()[feat], \n",
    "                          so()[feat], \n",
    "                          cci()[feat],\n",
    "                          wr()[feat],\n",
    "                          ic()[feat], \n",
    "                          rsi()[feat], \n",
    "                          lagged_px, \n",
    "                          lags,\n",
    "                          lagged_returns, \n",
    "                          period_returns,\n",
    "                          ir_d1], axis=1)\n",
    "    #day of the week and month may be useful features as well because of rebalancing flows\n",
    "    #we one-hot-encode them\n",
    "    features = pd.concat([features,\n",
    "                          pd.get_dummies(features.index.day_name()).set_index(features.index),\n",
    "                          pd.get_dummies(features.index.month_name()).set_index(features.index)], axis=1)\n",
    "    #features['day'], features['month'] = features.index.dayofweek, features.index.month\n",
    "    features['EOM']=features.index.map(lambda x: 1 if (offset.rollforward(x).day==x.day) else 0)\n",
    "    #features['EOQ']=features.index.map(lambda x: 1 if ((offset.rollforward(x).day==x.day) and (x.month in (3,6,9,12))) else 0)\n",
    "    #print('Applying fracDiff')\n",
    "    df = pd.concat([frac_diff_ffd(pd.DataFrame(features[c].dropna()), diff_amt=d, thresh=1e-5)  for c in tqdm(non_stationary_feat)], axis = 1)\n",
    "    df.columns = [f'{c}_frdif' for c in df]\n",
    "    \n",
    "    result = pd.concat([features,df,dailyVol, labels], axis = 1).dropna()\n",
    "    preprocessed_path= os.path.join(path_to_project, 'preprocessed_data', ''.join(ticker.split('/')) + '_feat.csv')\n",
    "    result.to_csv(preprocessed_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Now as we have all features ready we can start experimenting with machine learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
